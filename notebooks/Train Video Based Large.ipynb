{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDMARK_MODEL = \"landmark\"\n",
    "VISUAL_MODEL = \"visual\"\n",
    "COMBINED_MODEL = \"combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 5\n",
    "model_type = LANDMARK_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_words = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGJE7YkYEx7E"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23396,
     "status": "ok",
     "timestamp": 1698090635218,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "Pu8rW7RAEx7H",
    "outputId": "78cde691-a9a2-41eb-c76f-100bd457f648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in /mnt/iusers01/fse-ugpgt01/compsci01/h61781jp/.local/lib/python3.7/site-packages (0.5.12)\r\n",
      "Requirement already satisfied: docopt>=0.6.2 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from num2words) (0.6.2)\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.io import loadmat, savemat\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "\n",
    "!pip install num2words\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "## CSF\n",
    "proj_data_dir = \"../../proj_data/preprocessed\"\n",
    "proj_model_dir = \"../../proj_models/\"\n",
    "\n",
    "\n",
    "\n",
    "## Personal\n",
    "# proj_data_dir = \"D:/USB/data_gen_6/\"\n",
    "\n",
    "\n",
    "proj_code_dir = \"../classes\"\n",
    "sys.path.append(proj_code_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9247,
     "status": "ok",
     "timestamp": 1698090449623,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "CTiZybIWEx7J",
    "outputId": "56b5da71-0c48-4a07-e857-f9ab184fff3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 13:03:26.349095: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-21 13:03:26.351318: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-21 13:03:26.404146: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-21 13:03:26.405143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-21 13:03:30.014362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/95/ef83542e7a8e2bfc4432ee2cd8a6b52eb30fb1e605871e8871e94ce65fb1/datasets-2.13.2-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from datasets) (0.17)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/3f/8354ce12fd13bd5c5bb4722261a10ca1d6e2eb7c1c08fa3d8a4e9dc98f44/multiprocess-0.70.15-py37-none-any.whl\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/68/d3410e975bebbf5be00c1238d0418345d8ec5d88b7a6c102211a1c967edd/pyarrow-12.0.1.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aiohttp (from datasets)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl\n",
      "Collecting fsspec[http]>=2021.11.1 (from datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/64/f0d369ede0ca54fdd520bdee5086dbaf0af81dac53a2ce847bd1ec6e0bf1/fsspec-2023.1.0-py3-none-any.whl\n",
      "Collecting xxhash (from datasets)\n",
      "Requirement already satisfied: pandas in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from datasets) (0.24.2)\n",
      "Requirement already satisfied: packaging in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from datasets) (19.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from datasets) (2.22.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/be/e3/a84bf2e561beed15813080d693b4b27573262433fced9c1d1fea59e60553/dill-0.3.6-py3-none-any.whl\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/6d/ad/ff3b21ebfe79a4d25b4a4f8e5cf9fd44a204adb6b33c09010f566f51027a/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from datasets) (5.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (0.5.1)\n",
      "Collecting typing-extensions>=3.7.4; python_version < \"3.8\" (from aiohttp->datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from aiohttp->datasets) (19.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/a2/17e1e23c6be0a916219c5292f509360c345b5fa6beeb50d743203c27532c/multidict-6.0.5-py3-none-any.whl\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/18/3b0eb2690b3bf4d340a221d0e76b6c5f4cac9d5dd37fb8c7b6ec25c2f510/frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting asynctest==0.13.0; python_version < \"3.8\" (from aiohttp->datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/e8/b6/8d17e169d577ca7678b11cd0d3ceebb0a6089a7f4a2de4b945fe4b1c86db/asynctest-0.13.0-py3-none-any.whl\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/76/e6222113b83e3622caa4bb41032d0b1bf785250607392e1b778aca0b8a7d/charset_normalizer-3.3.2-py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached https://files.pythonhosted.org/packages/4d/05/4d79198ae568a92159de0f89e710a8d19e3fa267b719a236582eee921f4a/yarl-1.9.4-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from pandas->datasets) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from pandas->datasets) (2019.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from packaging->datasets) (2.4.0)\n",
      "Requirement already satisfied: six in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from packaging->datasets) (1.12.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Building wheels for collected packages: pyarrow\n",
      "  Building wheel for pyarrow (PEP 517) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Complete output from command /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/bin/python /opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/4790627.1.nvidiagpu.q/tmpe52e5v4p:\u001b[0m\n",
      "\u001b[31m  ERROR: running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-cpython-37\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/__init__.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_compute_docstrings.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_generated_version.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/acero.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/benchmark.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/cffi.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/compute.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/conftest.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/csv.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/cuda.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/dataset.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/feather.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/filesystem.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/flight.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/fs.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/hdfs.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/ipc.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/json.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/jvm.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/orc.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/pandas_compat.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/substrait.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/types.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/util.py -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/interchange\n",
      "  copying pyarrow/interchange/__init__.py -> build/lib.linux-x86_64-cpython-37/pyarrow/interchange\n",
      "  copying pyarrow/interchange/buffer.py -> build/lib.linux-x86_64-cpython-37/pyarrow/interchange\n",
      "  copying pyarrow/interchange/column.py -> build/lib.linux-x86_64-cpython-37/pyarrow/interchange\n",
      "  copying pyarrow/interchange/dataframe.py -> build/lib.linux-x86_64-cpython-37/pyarrow/interchange\n",
      "  copying pyarrow/interchange/from_dataframe.py -> build/lib.linux-x86_64-cpython-37/pyarrow/interchange\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/parquet\n",
      "  copying pyarrow/parquet/__init__.py -> build/lib.linux-x86_64-cpython-37/pyarrow/parquet\n",
      "  copying pyarrow/parquet/core.py -> build/lib.linux-x86_64-cpython-37/pyarrow/parquet\n",
      "  copying pyarrow/parquet/encryption.py -> build/lib.linux-x86_64-cpython-37/pyarrow/parquet\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/__init__.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/arrow_16597.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/arrow_7980.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/conftest.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/pandas_examples.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/pandas_threaded_import.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/read_record_batch.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/strategies.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_acero.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_array.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_builder.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_cffi.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_compute.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_convert_builtin.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_cpp_internals.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_csv.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_cuda.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_cython.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_dataset.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_deprecations.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_exec_plan.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_extension_type.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_feather.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_filesystem.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_flight.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_fs.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_gandiva.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_gdb.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_hdfs.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_io.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_ipc.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_json.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_jvm.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_memory.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_misc.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_orc.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_pandas.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_scalars.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_schema.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_sparse_tensor.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_strategies.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_substrait.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_table.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_tensor.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_types.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_udf.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/test_util.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/util.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/vendored\n",
      "  copying pyarrow/vendored/__init__.py -> build/lib.linux-x86_64-cpython-37/pyarrow/vendored\n",
      "  copying pyarrow/vendored/docscrape.py -> build/lib.linux-x86_64-cpython-37/pyarrow/vendored\n",
      "  copying pyarrow/vendored/version.py -> build/lib.linux-x86_64-cpython-37/pyarrow/vendored\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/tests/interchange\n",
      "  copying pyarrow/tests/interchange/__init__.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/interchange\n",
      "  copying pyarrow/tests/interchange/test_conversion.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/interchange\n",
      "  copying pyarrow/tests/interchange/test_interchange_spec.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/interchange\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/__init__.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/common.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/conftest.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/encryption.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_basic.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_compliant_nested_type.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_data_types.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_dataset.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_datetime.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_encryption.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_metadata.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_pandas.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/parquet\n",
      "  running egg_info\n",
      "  writing pyarrow.egg-info/PKG-INFO\n",
      "  writing dependency_links to pyarrow.egg-info/dependency_links.txt\n",
      "  writing requirements to pyarrow.egg-info/requires.txt\n",
      "  writing top-level names to pyarrow.egg-info/top_level.txt\n",
      "  /tmp/4790627.1.nvidiagpu.q/pip-install-vuw9mpm6/pyarrow/setup.py:34: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "  listing git files failed - pretending there aren't any\n",
      "  reading manifest file 'pyarrow.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching '../LICENSE.txt'\n",
      "  warning: no files found matching '../NOTICE.txt'\n",
      "  warning: no previously-included files matching '*.so' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "  warning: no previously-included files matching '#*' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.git*' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
      "  no previously-included directories found matching '.asv'\n",
      "  writing manifest file 'pyarrow.egg-info/SOURCES.txt'\n",
      "  copying pyarrow/__init__.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_acero.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_acero.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_compute.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_compute.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_csv.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_csv.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_cuda.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_cuda.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_dataset.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_dataset.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_dataset_orc.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_dataset_parquet.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_feather.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_flight.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_fs.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_fs.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_gcsfs.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_hdfs.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_hdfsio.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_json.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_orc.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_orc.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_parquet.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_parquet.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_parquet_encryption.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_parquet_encryption.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_pyarrow_cpp_tests.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_pyarrow_cpp_tests.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_s3fs.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/_substrait.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/array.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/benchmark.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/builder.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/compat.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/config.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/error.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/gandiva.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/io.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/ipc.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/lib.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/lib.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/memory.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/pandas-shim.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/public-api.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/scalar.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/table.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/tensor.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  copying pyarrow/types.pxi -> build/lib.linux-x86_64-cpython-37/pyarrow\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/__init__.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/common.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_acero.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_dataset_parquet.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_feather.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_flight.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_fs.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_python.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libarrow_substrait.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/includes/libgandiva.pxd -> build/lib.linux-x86_64-cpython-37/pyarrow/includes\n",
      "  copying pyarrow/tests/bound_function_visit_strings.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/extensions.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.linux-x86_64-cpython-37/pyarrow/tests\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/src\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/CMakeLists.txt -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/api.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/arrow_to_pandas.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/arrow_to_pandas.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/arrow_to_python_internal.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/benchmark.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/benchmark.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/common.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/common.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/csv.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/csv.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/datetime.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/datetime.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/decimal.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/decimal.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/deserialize.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/deserialize.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/extension_type.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/extension_type.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/filesystem.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/filesystem.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/flight.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/flight.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/gdb.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/gdb.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/helpers.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/helpers.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/inference.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/inference.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/init.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/init.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/io.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/io.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/ipc.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/ipc.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/iterators.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/numpy_convert.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/numpy_convert.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/numpy_internal.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/numpy_interop.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/numpy_to_arrow.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/numpy_to_arrow.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/parquet_encryption.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/parquet_encryption.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/pch.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/platform.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/pyarrow.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/pyarrow.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/pyarrow_api.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/pyarrow_lib.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/python_test.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/python_test.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/python_to_arrow.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/python_to_arrow.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/serialize.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/serialize.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/type_traits.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/udf.cc -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/udf.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  copying pyarrow/src/arrow/python/visibility.h -> build/lib.linux-x86_64-cpython-37/pyarrow/src/arrow/python\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/tests/data\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/feather\n",
      "  copying pyarrow/tests/data/feather/v0.17.0.version.2-compression.lz4.feather -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/feather\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/README.md -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  copying pyarrow/tests/data/orc/decimal.orc -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/orc\n",
      "  creating build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/parquet\n",
      "  copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/parquet\n",
      "  copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/parquet\n",
      "  copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/parquet\n",
      "  copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.linux-x86_64-cpython-37/pyarrow/tests/data/parquet\n",
      "  running build_ext\n",
      "  creating /tmp/4790627.1.nvidiagpu.q/pip-install-vuw9mpm6/pyarrow/build/temp.linux-x86_64-cpython-37\n",
      "  -- Running cmake for PyArrow\n",
      "  cmake -DCMAKE_INSTALL_PREFIX=/tmp/4790627.1.nvidiagpu.q/pip-install-vuw9mpm6/pyarrow/build/lib.linux-x86_64-cpython-37/pyarrow -DPYTHON_EXECUTABLE=/opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/bin/python -DPython3_EXECUTABLE=/opt/apps/apps/binapps/anaconda3/2019.07-numpy-fix/bin/python -DPYARROW_CXXFLAGS= -DPYARROW_BUILD_CUDA=off -DPYARROW_BUILD_SUBSTRAIT=off -DPYARROW_BUILD_FLIGHT=off -DPYARROW_BUILD_GANDIVA=off -DPYARROW_BUILD_ACERO=off -DPYARROW_BUILD_DATASET=off -DPYARROW_BUILD_ORC=off -DPYARROW_BUILD_PARQUET=off -DPYARROW_BUILD_PARQUET_ENCRYPTION=off -DPYARROW_BUILD_GCS=off -DPYARROW_BUILD_S3=off -DPYARROW_BUILD_HDFS=off -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_CYTHON_CPP=off -DPYARROW_GENERATE_COVERAGE=off -DCMAKE_BUILD_TYPE=release /tmp/4790627.1.nvidiagpu.q/pip-install-vuw9mpm6/pyarrow\n",
      "  error: command 'cmake' failed: No such file or directory: 'cmake'\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for pyarrow\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pyarrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build pyarrow\n",
      "\u001b[31mERROR: Could not build wheels for pyarrow which use PEP 517 and cannot be installed directly\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/iusers01/fse-ugpgt01/compsci01/h61781jp/.conda/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, LSTM, TimeDistributed,Bidirectional, InputLayer,Attention,Dense,Dropout,Input,Embedding,MultiHeadAttention,LayerNormalization, Concatenate, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "!pip install datasets\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLDmvm2GEx7K"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_frames = []\n",
    "train_X_landmarks = []\n",
    "train_Y = []\n",
    "\n",
    "\n",
    "test_X_frames = []\n",
    "test_X_landmarks = []\n",
    "test_Y = []\n",
    "\n",
    "val_X_frames = []\n",
    "val_X_landmarks = []\n",
    "val_Y = []\n",
    "\n",
    "data_mats = os.listdir(proj_data_dir)\n",
    "for path in os.listdir(proj_data_dir):\n",
    "    path = os.path.join(proj_data_dir, path)\n",
    "    \n",
    "    data_mat = loadmat(path)\n",
    "    \n",
    "    train_X_frames.extend(data_mat[\"train_data_frames\"])\n",
    "    train_X_landmarks.extend(data_mat[\"train_data_keypoints\"])\n",
    "    train_Y.extend(data_mat[\"train_labels\"])\n",
    "\n",
    "\n",
    "    test_X_frames.extend(data_mat[\"test_data_frames\"])\n",
    "    test_X_landmarks.extend(data_mat[\"test_data_keypoints\"])\n",
    "    test_Y.extend(data_mat[\"test_labels\"])\n",
    "\n",
    "    val_X_frames.extend(data_mat[\"val_data_frames\"])\n",
    "    val_X_landmarks.extend(data_mat[\"val_data_keypoints\"])\n",
    "    val_Y.extend(data_mat[\"val_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data():\n",
    "    print(f\"Train data frames length: {np.shape(train_X_frames)}\")\n",
    "    print(f\"Train data landmarks length: {np.shape(train_X_landmarks)}\")\n",
    "    print(f\"Train labels length: {np.shape(train_Y)}\")\n",
    "\n",
    "    print(f\"Test data frames length: {np.shape(test_X_frames)}\")\n",
    "    print(f\"Test data landmarks length: {np.shape(test_X_landmarks)}\")\n",
    "    print(f\"Test labels length: {np.shape(test_Y)}\")\n",
    "\n",
    "    print(f\"Validation data frames length: {np.shape(val_X_frames)}\")\n",
    "    print(f\"Validation data landmarks length: {np.shape(val_X_landmarks)}\")\n",
    "    print(f\"Validation labels length: {np.shape(val_Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = ['ABOUT', 'AGAIN', 'AHEAD', 'ARRESTED', 'BELIEVE', 'BUSINESS', 'CHANCE', 'DESPITE', 'ELECTION', 'FAMILY', 'FIGURES', 'FOREIGN', 'FRANCE', 'GIVING', 'LATER', 'LEADERS', 'MIGHT', 'MINUTES', 'NEEDS', 'NOTHING', 'OFFICIALS', 'PAYING', 'POSSIBLE', 'POWERS', 'RIGHTS', 'SITUATION', 'STREET', 'TALKING', 'THIRD', 'THROUGH']\n",
    "num_words = len(word_set)\n",
    "\n",
    "backward_word_translations = {i: word_set[i] for i in range(num_words)}\n",
    "print(backward_word_translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_subset = ['ABOUT', 'BELIEVE', 'BUSINESS', 'CHANCE', 'DESPITE', 'ELECTION', 'FAMILY' ,'GIVING', 'LATER', 'MIGHT', 'NOTHING', 'POSSIBLE', 'SITUATION','THROUGH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(frames, landmarks, labels):\n",
    "    temp_frames = []\n",
    "    temp_landmarks = []\n",
    "    temp_labels = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        true = backward_word_translations[np.argmax(labels[i])]\n",
    "        if(true in word_subset):\n",
    "            temp_frames.append(frames[i])\n",
    "            temp_landmarks.append(landmarks[i])\n",
    "            temp_labels.append(labels[i])\n",
    "\n",
    "    return (temp_frames, temp_landmarks, temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(filter_words):\n",
    "    train_X_frames, train_X_landmarks, train_Y = filter_samples(train_X_frames, train_X_landmarks, train_Y)\n",
    "    test_X_frames, test_X_landmarks, test_Y = filter_samples(test_X_frames, test_X_landmarks, test_Y)\n",
    "    val_X_frames, val_X_landmarks, val_Y = filter_samples(val_X_frames, val_X_landmarks, val_Y)\n",
    "    \n",
    "    num_words = len(word_subset)\n",
    "\n",
    "    backward_word_translations = {i: word_subset[i] for i in range(num_words)}\n",
    "    print(backward_word_translations)\n",
    "    \n",
    "    train_Y = np.array(train_Y).T[:][:num_words].T\n",
    "    test_Y = np.array(test_Y).T[:][:num_words].T\n",
    "    val_Y = np.array(val_Y).T[:][:num_words].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJC-4V11Ex7N"
   },
   "source": [
    "## Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698090825620,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "-OCgdhnFEx7N"
   },
   "outputs": [],
   "source": [
    "def landmarks_to_img(landmarks):\n",
    "  width = 200\n",
    "  height = 200\n",
    "  new_frame = np.zeros((width,height))\n",
    "  for ldmk in landmarks:\n",
    "      new_frame = cv2.circle(\n",
    "          img=new_frame,\n",
    "          center=(int(ldmk[0] * width), int(ldmk[1] * height)),\n",
    "          radius=2,\n",
    "          thickness=1,\n",
    "          color=(255,255,255),\n",
    "      )\n",
    "  return new_frame\n",
    "\n",
    "def show_frames(frames, landmark: bool):\n",
    "  print(np.shape(frames))\n",
    "  fig, ax = plt.subplots(figsize=(18, 2))\n",
    "  if(not landmark):\n",
    "    row = frames[0]\n",
    "    for x in range(1,len(frames)):\n",
    "      row = np.concatenate((row, frames[x]), axis = 1)\n",
    "    ax.imshow(row, cmap='gray', vmin=0, vmax=255)\n",
    "  else:\n",
    "    row = landmarks_to_img(frames[0])\n",
    "    for x in range(1,len(frames)):\n",
    "      row = np.concatenate((row, landmarks_to_img(frames[x])), axis = 1)\n",
    "    ax.imshow(row, cmap='gray', vmin=0, vmax=255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1698090825955,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "-T7--vpYEx7N",
    "outputId": "fde89283-0146-4789-bbda-e16d76a02866"
   },
   "outputs": [],
   "source": [
    "show_frames(frames=train_X_landmarks[0],landmark=True)\n",
    "print(f\"Class: {train_Y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2zVJVheEx7O"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.0001\n",
    "epochs = 100\n",
    "batch_size: int = 32\n",
    "decay_rate: float = 0.95\n",
    "\n",
    "decay_steps = int(len(train_Y)/batch_size)\n",
    "print(f\"decay_steps: {decay_steps}\")\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(proj_model_dir, \"best_run.hdf5\"),\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_lr,decay_steps=decay_steps,decay_rate=decay_rate,staircase=False)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if(epoch % 20 == 0):\n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr\n",
    "schedule_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh6xKd9mEx7O"
   },
   "source": [
    "Model is video based & word based. It uses feature vectors extracted for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(model_type == COMBINED_MODEL):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      ((train_X_frames,train_X_landmarks), train_Y)\n",
    "    )\n",
    "    train_ds = train_ds.padded_batch(batch_size, padded_shapes=((np.shape(train_X_frames[0]),np.shape(train_X_landmarks[0])),np.shape(train_Y[0])))\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      ((test_X_frames,test_X_landmarks), test_Y)\n",
    "    )\n",
    "    test_ds = test_ds.padded_batch(batch_size, padded_shapes=((np.shape(test_X_frames[0]),np.shape(test_X_landmarks[0])),np.shape(test_Y[0])))\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      ((val_X_frames,val_X_landmarks), val_Y)\n",
    "    )\n",
    "    val_ds = val_ds.padded_batch(batch_size, padded_shapes=((np.shape(val_X_frames[0]),np.shape(val_X_landmarks[0])),np.shape(val_Y[0])))\n",
    "elif(model_type == VISUAL_MODEL):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (train_X_frames, train_Y)\n",
    "    )\n",
    "    train_ds = train_ds.padded_batch(batch_size, padded_shapes=(np.shape(train_X_frames[0]),np.shape(train_Y[0])))\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (test_X_frames, test_Y)\n",
    "    )\n",
    "    test_ds = test_ds.padded_batch(batch_size, padded_shapes=(np.shape(test_X_frames[0]),np.shape(test_Y[0])))\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (val_X_frames, val_Y)\n",
    "    )\n",
    "    val_ds = val_ds.padded_batch(batch_size, padded_shapes=(np.shape(val_X_frames[0]),np.shape(val_Y[0])))\n",
    "elif(model_type == LANDMARK_MODEL):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (train_X_landmarks, train_Y)\n",
    "    )\n",
    "    train_ds = train_ds.padded_batch(batch_size, padded_shapes=(np.shape(train_X_landmarks[0]),np.shape(train_Y[0])))\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (test_X_landmarks, test_Y)\n",
    "    )\n",
    "    test_ds = test_ds.padded_batch(batch_size, padded_shapes=(np.shape(test_X_landmarks[0]),np.shape(test_Y[0])))\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (val_X_landmarks, val_Y)\n",
    "    )\n",
    "    val_ds = val_ds.padded_batch(batch_size, padded_shapes=(np.shape(val_X_landmarks[0]),np.shape(val_Y[0])))\n",
    "    \n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1698091267139,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "qgxaEyGCEx7P",
    "outputId": "8507aedf-8bec-498d-b091-3ce41e7a0674"
   },
   "outputs": [],
   "source": [
    "if(3 <= model_num <= 5):\n",
    "    input_shape = np.shape(train_X_frames[0])\n",
    "    if(model_num == 5):\n",
    "        input_shape = np.shape(train_X_landmarks[0])\n",
    "    model = Sequential([\n",
    "       layers.InputLayer(input_shape= input_shape),\n",
    "       layers.TimeDistributed(Flatten()),\n",
    "       layers.Bidirectional(LSTM(128,activation='relu', return_sequences=True)),\n",
    "       layers.Dropout(0.2),\n",
    "       layers.Bidirectional(LSTM(128,activation='relu', return_sequences=True)),\n",
    "       layers.Dropout(0.2),\n",
    "       layers.Bidirectional(LSTM(128,activation='relu', return_sequences=False)),\n",
    "       layers.Dense(64,activation='relu'),\n",
    "       layers.Dropout(0.2),\n",
    "       layers.Dense(32,activation='relu'),\n",
    "       layers.Dropout(0.2),\n",
    "       layers.Dense(16,activation='relu'),\n",
    "       layers.Dropout(0.2),\n",
    "       layers.Dense(num_words,activation='softmax')\n",
    "    ])\n",
    "elif(model_num == 8):\n",
    "    # ## Both attention architecture\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    input_layer_frames = Input(shape= np.shape(train_X_frames[0]), name=\"input_frames\")\n",
    "    flatten_layer_frames = TimeDistributed(Flatten())(input_layer_frames)\n",
    "    attention_layer_frames = Attention(use_scale=True)([flatten_layer_frames,flatten_layer_frames])\n",
    "    norm_layer_frames = LayerNormalization(epsilon=epsilon)(flatten_layer_frames + attention_layer_frames)\n",
    "\n",
    "    input_layer_landmarks = Input(shape= np.shape(train_X_landmarks[0]), name=\"input_landmarks\")\n",
    "    flatten_layer_landmarks = TimeDistributed(Flatten())(input_layer_landmarks)\n",
    "    dense_scale_layer = Dense(np.shape(train_X_frames[0])[-1])(flatten_layer_landmarks)\n",
    "    attention_layer_landmarks = Attention(use_scale=True)([dense_scale_layer,dense_scale_layer])\n",
    "    norm_layer_landmarks = LayerNormalization(epsilon=epsilon)(dense_scale_layer + attention_layer_landmarks)\n",
    "\n",
    "    cross_attention_layer1 = Attention(use_scale=True)([norm_layer_landmarks,norm_layer_frames])\n",
    "    cross_norm_layer1 = LayerNormalization(epsilon=epsilon)(cross_attention_layer1 + norm_layer_landmarks + norm_layer_frames)\n",
    "\n",
    "    cross_attention_layer2 = Attention(use_scale=True)([norm_layer_frames,norm_layer_landmarks])\n",
    "    cross_norm_layer2 = LayerNormalization(epsilon=epsilon)(cross_attention_layer2 + norm_layer_landmarks + norm_layer_frames)\n",
    "\n",
    "    concatenate_layer = Concatenate(axis=-1)([cross_norm_layer1,cross_norm_layer2])\n",
    "\n",
    "    conv_layer1 = Conv1D(1024, 3, padding='same', activation='relu')(concatenate_layer)\n",
    "    dropout1 = Dropout(0.1)(conv_layer1)\n",
    "    max_pooling1 = MaxPooling1D()(dropout1)\n",
    "\n",
    "    conv_layer2 = Conv1D(256, 3, padding='same', activation='relu')(max_pooling1)\n",
    "    dropout2 = Dropout(0.1)(conv_layer2)\n",
    "    max_pooling2 = MaxPooling1D()(dropout2)\n",
    "\n",
    "    conv_layer3 = Conv1D(64, 3, padding='same', activation='relu')(max_pooling2)\n",
    "    dropout3 = Dropout(0.1)(conv_layer3)\n",
    "    max_pooling3 = MaxPooling1D()(dropout3)\n",
    "\n",
    "    conv_layer4 = Conv1D(32, 3, padding='same', activation='relu')(max_pooling3)\n",
    "    max_pooling4 = MaxPooling1D()(conv_layer4)\n",
    "\n",
    "    flatten_layer_final = Flatten()(max_pooling4)\n",
    "\n",
    "    output_layer = Dense(num_words,activation='softmax')(flatten_layer_final)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[input_layer_frames,input_layer_landmarks], outputs=output_layer)\n",
    "elif(model_num == 7):\n",
    "    #Transformer architecture\n",
    "    sequence_length = len(train_X_landmarks[0])\n",
    "    d_model = 120\n",
    "    num_encoder_layers = 3\n",
    "    num_heads = 3\n",
    "    dff = 120\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    input_layer = Input(shape= np.shape(train_X_landmarks[0]))\n",
    "    flatten_layer1 = TimeDistributed(Flatten())(input_layer)\n",
    "\n",
    "    positional_encoding = keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=d_model\n",
    "        )(tf.range(start=0, limit=sequence_length, delta=1))\n",
    "\n",
    "    add_layer = flatten_layer1 + positional_encoding\n",
    "\n",
    "    for _ in range(num_encoder_layers):\n",
    "        x = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(add_layer, add_layer)\n",
    "        x = LayerNormalization(epsilon=epsilon)(x + add_layer)\n",
    "\n",
    "        ffn = keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        y = ffn(x)\n",
    "        add_layer = LayerNormalization(epsilon=epsilon)(y + x)\n",
    "\n",
    "    flatten_layer2 = Flatten()(x)\n",
    "    output_layer = Dense(num_words, activation='softmax')(flatten_layer2)\n",
    "\n",
    "    model = keras.Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1698091267139,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "LAp04lG7Ex7P"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1698091267139,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "BzzzxyMwEx7P",
    "outputId": "50f6ee3d-ac9a-460e-9874-48c21853c619"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-VepzV3Ex7P"
   },
   "source": [
    "## Example Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 999,
     "status": "ok",
     "timestamp": 1698091268134,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "YUMqqPpBEx7P",
    "outputId": "2fa40a07-8f91-43d2-8822-0c5b430cd124"
   },
   "outputs": [],
   "source": [
    "print(f\"Translations: {backward_word_translations}\")\n",
    "\n",
    "\n",
    "test_sample_frame = train_X_frames[0]\n",
    "test_sample_frame = tf.expand_dims(test_sample_frame, 0)\n",
    "    \n",
    "test_sample_landmarks = train_X_landmarks[0]\n",
    "test_sample_landmarks = tf.expand_dims(test_sample_landmarks, 0)\n",
    "if(model_type == COMBINED_MODEL):\n",
    "    test_sample = [test_sample_frame, test_sample_landmarks]\n",
    "elif(model_type == LANDMARK_MODEL):\n",
    "    test_sample = test_sample_landmarks\n",
    "elif(model_type == VISUAL_MODEL):\n",
    "    test_sample = test_sample_frame\n",
    "\n",
    "test_label = train_Y[0]\n",
    "\n",
    "print(f\"Model input shape: {model.input_shape}\")\n",
    "test_out = model.predict(test_sample)\n",
    "predicted = tf.argmax(test_out,axis=1)\n",
    "\n",
    "print(f\"Predicted: {test_out}\")\n",
    "print(f\"True: {test_label}\")\n",
    "\n",
    "print(f\"Translated Predicted: {backward_word_translations[int(predicted[0])]}\")\n",
    "print(f\"Translated True: {backward_word_translations[tf.argmax(test_label).numpy()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZijKIcSEx7P"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1391701,
     "status": "ok",
     "timestamp": 1698093072901,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "Wycr1zjaEx7Q",
    "outputId": "e065a4cb-9876-45fe-f95e-07af62e70869"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks=[model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UUx1dYHEx7Q"
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1698093117757,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "U1suKBuMEx7Q"
   },
   "outputs": [],
   "source": [
    "# model.save(os.path.join(proj_model_dir, \"newest_model.keras\"))\n",
    "\n",
    "model.save_weights(os.path.join(proj_model_dir, \"newest_model.hdf5\"))\n",
    "model_architecture = model.to_json()\n",
    "with open(os.path.join(proj_model_dir, \"best_run.json\"), \"w\") as f:\n",
    "  f.write(json.dumps(model_architecture, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faQaJMqIEx7Q"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfgSbcNMEx7T"
   },
   "source": [
    "## Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1698093151500,
     "user": {
      "displayName": "Jack Pay",
      "userId": "06088299984570525080"
     },
     "user_tz": -60
    },
    "id": "bLLR1ETiEx7T",
    "outputId": "598a1649-7c2b-4e7c-984e-f5bddc6fa974"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Test Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Test Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.savefig(os.path.join(proj_model_dir, \"newest_model_metrics.pdf\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proj_model_dir = \"../models/2nd experiment/\"\n",
    "\n",
    "with open(os.path.join(proj_model_dir, \"best_run.json\"), \"r\") as f:\n",
    "    model_architecture = json.loads(f.read())\n",
    "\n",
    "best_model = keras.models.model_from_json(model_architecture)\n",
    "best_model.load_weights(os.path.join(proj_model_dir, \"best_run.hdf5\"))\n",
    "best_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result = best_model.evaluate(val_ds)\n",
    "\n",
    "loss = val_result[0]\n",
    "accuracy = val_result[1]\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# targets = []\n",
    "# for batch in val_ds:\n",
    "#     X, y = batch\n",
    "#     batch_predictions = best_model.predict(X)\n",
    "#     predicted = tf.argmax(batch_predictions,axis=1)\n",
    "    \n",
    "#     batch_predictions = [backward_word_translations[index] for index in predicted]\n",
    "#     batch_targets = [backward_word_translations[index] for index in tf.argmax(y,axis=1)]\n",
    "    \n",
    "#     predictions.extend(batch_predictions)\n",
    "#     targets.extend(batch_targets)\n",
    "\n",
    "# wer_score = wer(targets, predictions)\n",
    "# print(\"-\" * 100)\n",
    "# print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "# print(\"-\" * 100)\n",
    "# for i in range(0, len(predictions)):\n",
    "#     print(f\"Target    : {targets[i]}\")\n",
    "#     print(f\"Prediction: {predictions[i]}\")\n",
    "#     print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
